
---
title: "Vignette for simpleEnsembleGroup22 Package"
author: "Sravya Yarlagadda, Hasan Md Moonam, Aman Arya, Eric Seeram"
date: "`r Sys.Date()`"
output:
  pdf_document: 
    toc: true
    toc_depth: 3
    number_sections: true
    df_print: paged
---

## Introduction

This vignette describes the usage and internal workings of the `simpleEnsembleGroup22` package, which includes various techniques for regression and ensemble learning.

```{r setup, include=FALSE, results='hide', message=FALSE, echo=FALSE}
options(max.print = 5) 
options(repos = "https://cran.r-project.org")

```
## Importing the Package

To use the `simpleEnsembleGroup22` package, you will have to install the package like this specifying the location of the package. 

```{r installing package}

install.packages("C:/Users/sryarlagadda/Downloads/simpleEnsembleGroup22_0.0.0.9000.tar.gz")
library("simpleEnsembleGroup22")

```
## Internal Workings of Various Functions and Examples

This package is better compatible when the function simple_ensemble() is used to specify the requirements and input variables (The dataset, whether ensembling of model types is required, option for bagging on a single model type, option to select top K predictors of the dataset to perform model fitting). The datasets used should have the response variables as binary or continuous and the explanatory variables can be a combination of binary, discrete and continuous variables. Please ensure that the dataset doesn't contain any missing values.

Disclaimer: The package also provides the following functions individually. The datasets and other parameters have to be given as inputs to the functions in the format mentioned in the examples. Checks like this have to be ensured - For a simple linear model, number of predictors has to be less than the sample size. For other models, the sample size has to be atleast 200. For These are not compatible for datasets with missing values. The predicted values here are the fitted values of the models. 

1. Fitting various models for a dataset (linear_model(), ridge_model(), lasso_model(), elastic_net_regression(), rf_model()). 

2. Ensemble Learning of various models - "elastic_net" and "random_forest". (Since elastic_net model is an ensemble of ridge and lasso models, and since ridge and lasso are penalized linear regression methods) More on this can be found from the ensemble_predict() documentation.

3. Bagging of a single model type - The dataset is sampled using replacement and the predictions of these multiple models are averaged.

4. Screen for top K predictors of a dataset. Models are usually suitable to be fitted when n>>p (n=sample size, p=number of predcitor variables). This function screens for top K predictors(if specified by the user or if n<20*p, it screens for top K predictors based on a certain threshold value). More on this can be known from the variable_pre_screening() documentation.

### 1. Linear Regression Model (`linear_model`)

The linear_model function fits a linear model using either linear regression for continuous response variables or logistic regression for binary response variables.

Description:

This function accepts two arguments:

X: Matrix of predictors .
   Each row represents an observation, and each column represents a predictor. It can be a mix of continuous, discrete, and binary predictors.
   
y: Response variable.
   For binary outcomes, it should be a binary vector (0 or 1). For continuous outcomes, it should be a numeric vector.
   
It returns a list containing the fitted model and predicted values.

Details:

For continuous response variables, linear_model performs linear regression.

For binary response variables, it performs logistic regression.

**Examples**:

```{r linear-example, message=FALSE, collapse=TRUE, max.lines=5}

library(MASS)
data(Boston)
str(Boston) 
X <- Boston[, 2:14]
y <- Boston[,1]
linear_results <- linear_model(X, y) #For Continuous Response variable
linear_results

data(Pima.te)  
str(Pima.te)
X <- Pima.te[, 1:7]
y <- Pima.te[,8]
logistic_results <- linear_model(X, as.numeric(y) - 1) #For Binary Response variable
logistic_results

```

### 2. Ridge Regression (`ridge_model`)

The ridge_model function performs ridge regression, which is a regularization technique used to mitigate multicollinearity in the data and prevent overfitting.

Ridge regression adds an L2 penalty term to the standard linear regression objective function, minimizing the sum of squared residuals plus a penalty term that shrinks the coefficients towards zero. The function performs cross-validation to select the optimal lambda value, which controls the amount of regularization applied.We have used 10-fold cross-validation as the standard here. It fits the final ridge regression model using the optimal lambda value.

Description:

This function accepts two arguments:

X: Matrix of predictors . Each row represents an observation, and each column represents a predictor. It can be a mix of continuous, discrete, and binary predictors.

y: Response variable. For binary outcomes, it should be a binary vector (0 or 1). For continuous outcomes, it should be a numeric vector.

Value:

The function returns a list containing two elements:

model: Fitted ridge regression model.

predicted.value: Predicted values based on the fitted model.

**Example**:

```{r ridge-example}
library(MASS)
data(Boston)
str(Boston) 
X <- Boston[, 2:14]
y <- Boston[,1]
ridge_results <- ridge_model(X, y) #For Continuous Response variable
ridge_results

data(Pima.te) 
str(Pima.te)
X <- Pima.te[, 1:7]
y <- Pima.te[,8]
ridge_results <- ridge_model(X, as.numeric(y) - 1) #For Binary Response variable
ridge_results
```

### 3. Lasso Regression (`lasso_model`)

The lasso_model function fits a Lasso regression model where an L1 penalty is applied to the standard linear regression objective function. L1 penalty has the effect of forcing some of the coefficient estimates to be exactly equal to zero, which means this model is type of variable selection. The function using the glmnet package with cross-validation to select the optimal lambda value. We have used 10-fold cross-validation as the standard here.

Description:

This function accepts three arguments:

X: Matrix of predictors . Each row represents an observation, and each column represents a predictor. It can be a mix of continuous, discrete, and binary predictors.

y: Response variable. For binary outcomes, it should be a binary vector (0 or 1). For continuous outcomes, it should be a numeric vector.

It returns a list containing the fitted model and predicted values.

**Example**:

```{r lasso-example}
library(MASS)
data(Boston)
str(Boston) 
X <- Boston[, 2:14]
y <- Boston[,1]
lasso_results <- lasso_model(X, y) #For Continuous Response variable
lasso_results

data(Pima.te) 
str(Pima.te)
X <- Pima.te[, 1:7]
y <- Pima.te[,8]
lasso_results <- lasso_model(X, as.numeric(y) - 1) #For Binary Response variable
lasso_results
```
### 4. Elastic Net Regression (`elastic_net_regression`)

The elastic_net_regression function fits an elastic net regression model using the glmnet package with cross-validated alpha selection. It selects an optimal alpha value (The alpha parameter in elastic net regression controls the balance between the L1 (Lasso) and L2 (Ridge) penalties in the regularization term. A value of 1 corresponds to pure Lasso regression, while a value of 0 corresponds to pure Ridge regression. Values between 0 and 1 represent a mixture of Lasso and Ridge penalties).

Description:

This function accepts two arguments:

X: Matrix of predictors . Each row represents an observation, and each column represents a predictor. It can be a mix of continuous, discrete, and binary predictors.

y: Response variable. For binary outcomes, it should be a binary vector (0 or 1). For continuous outcomes, it should be a numeric vector.

It returns a list containing three elements:

model: Fitted elastic net regression model object.

optimal.alpha: Optimal alpha value selected through cross-validation.

predicted.value: Predicted values based on the fitted model.

**Example**:

```{r elastic-net-example}
library(MASS)
data(Boston)
str(Boston) 
X <- Boston[, 2:14]
y <- Boston[,1]
elastic_net_results <- elastic_net_regression(X, y) #For Continuous Response variable
elastic_net_results

data(Pima.te) 
str(Pima.te)
X <- Pima.te[, 1:7]
y <- Pima.te[,8]
elastic_net_results <- elastic_net_regression(X, as.numeric(y) - 1) #For Binary Response variable
elastic_net_results
```

### 5. Random Forest (`rf_model`)

A Random Forest is machine learning model that combines the predictions of multiple decision trees to improve the overall predictive accuracy and reduce overfitting.

The number of trees (ntree) and the number of variables randomly sampled as candidates at each split (mtry) are set to default values. For classification tasks, mtry is set to the square root of the number of predictors (sqrt(ncol(X))), and for regression tasks, it is set to one-third of the number of predictors (ncol(X) / 3). These are default values.

Description:

This function accepts two arguments:

X: Matrix of predictors . Each row represents an observation, and each column represents a predictor. It can be a mix of continuous, discrete, and binary predictors.

y: Response variable. For binary outcomes, it should be a binary vector (0 or 1). For continuous outcomes, it should be a numeric vector.

The function returns a fitted Random Forest model object.

**Example**:

```{r rf-example}
library(MASS)
data(Boston)
str(Boston) 
X <- Boston[, 2:14]
y <- Boston[,1]
rf_results <- rf_model(X, y) #For Continuous Response variable
rf_results
rf_results$predicted
rf_results$importance

data(Pima.te) 
str(Pima.te)
X <- Pima.te[, 1:7]
y <- Pima.te[,8]
rf_results <- rf_model(X, as.numeric(y) - 1) #For Binary Response variable
rf_results
rf_results$predicted
rf_results$importance
```

### 6. Bagging (`bagged_model`)

This function enables bagging with different machine learning models. Bagging, short for Bootstrap Aggregating, is an ensemble learning technique that improves the stability and accuracy of models by training multiple models(of a single type) on different subsets of the training data and combining their predictions.

Description:

This function accepts the following arguments:

- X: Matrix of predictors. Each row represents an observation, and each column represents a predictor. It can include a mix of continuous, discrete, and binary predictors.

- y: Response variable. For binary outcomes, it should be a binary vector (0 or 1). For continuous outcomes, it should be a numeric vector.

- r.bagging: Number of bagging iterations.

- modelType: Type of machine learning model to use for bagging. Options include "linear", "ridge", "lasso", "elastic_net", and "random_forest". If the model type is "random_forest", the function warns the user that further bagging is not allowed since random forests already use bagging internally.

The function iteratively performs bagging by resampling the original dataset, fitting the model to each resampled dataset, and aggregating the predictions (In case of continuous response variable, taking the simple mean and n case of binary response variable, majority vote). 

Value:

The function returns a list with the following elements:

- fitted.values: Predicted values based on the fitted bagged model.

- naive: A vector containing the count of naive variables selected in each bagging. (In case of a ridge model, the naive value is not needed, because all the variables are selected when model is fit for each resampling.)

**Examples**:

```{r bagging-example}
library(MASS)
data(Boston)
str(Boston) 
X <- Boston[, 2:14]
y <- Boston[,1]
bagged_results <- bagged_model(X, y, r.bagging = 10, modelType = "linear")
print(bagged_results)
```

### 7. Ensemble Learning (`ensemble_model`)

Ensemble learning is a powerful technique in machine learning where multiple models are combined to improve overall performance. The intuition behind ensemble learning is that by combining the predictions of multiple models, we can reduce the risk of making a wrong prediction and often achieve better results than any individual model.

Here we are using 2 models as base learners - Elastic Net provides regularization and variable selection capabilities, while Random Forest offers robustness and non-linear modeling capabilities.

Description:

This function accepts two arguments:

X: Matrix of predictors. Each row represents an observation, and each column represents a predictor. It can be a mix of continuous, discrete, and binary predictors.

y: Response variable. For binary outcomes, it should be a binary vector (0 or 1). For continuous outcomes, it should be a numeric vector.

The function returns a list containing the predictions and accuracies of the individual models and the ensemble model(
ensembled_prediction). (For calculating the predictions - In case of continuous response variable, it is taking the simple mean and in case of binary response variable, it takes majority vote.)

elastic.net: List containing information about the Elastic Net model, including predicted values.

random.forest: List containing information about the Random Forest model, including predicted values.

**Example**:

```{r ensemble-example}
library(MASS)
data(Boston)
str(Boston) 
X <- Boston[, 2:14]
y <- Boston[,1]
ensemble_predictions <- ensemble_predict(X, y) #For Continuous Response variable
ensemble_predictions

data(Pima.te) 
str(Pima.te)
X <- Pima.te[, 1:7]
y <- Pima.te[,8]
ensemble_predictions <- ensemble_predict(X, as.numeric(y) - 1) #For Binary Response variable
ensemble_predictions
```

### 8. Variable Screening (`variable_screening`)

Variable Screening for Top Predictors

This function performs Univariate filtering (Univariate filtering evaluates each feature's individual association with
the target variable through statistical tests, selecting the most significant features based on measures such as correlation, ANOVA, or chi-squared tests.) on each column of the predictor matrix X based on the provided output variable y.

Various statistical tests are applied depending on the data types of X and y to identify significant predictors - 

If y is numeric and X is numeric, Pearson's correlation test is applied.

If y is numeric and X is binary,  Kruskal-Wallis test or Simple Linear Regression is used.

If y is binary and X is numeric, ANOVA or Fisher's Exact Test is used.

If both y and X are binary, Chi-squared test is applied.

Arguments

X: Matrix of predictors. Each row represents an observation, and each column represents a predictor. It can be a mix of continuous, discrete, and binary predictors.

y: Response variable. Can be continuous and normally distributed or binary responses.

K: Number of top predictors to select. If not specified, default is half the number of original number of predictor variables.

This returns a dataframe containing the selected predictor variables in order of significance (if pValue for the above tests are <0.05, then ordering is based on this value, i.e. lesser pValue => more significant predictor)

**Example**:

```{r variable-screening-example}
library(MASS)
data(survey)
str(survey)
X <- survey[, -10] 
y <- survey$Height
selected_variables <- variable_pre_screening(X, y, 4) 
print(selected_variables)
```
### 9. Combining Ensemble, Bagging, Models and Feature Selection (`simple_ensemble`)

The simple_ensemble function allows users to specify a set of models to include in the ensemble and to give option for top K number of predictors to be selected for the model (By default, if n<20*p, where n=sample size and p=number of explanatory variables, it selects for some top predictors cross a certain threshold value for their univariate pValue), allows users to give the number of times to sample to perform bagging for each model, and whether to perform ensemble learning of the models.

Function Arguments

X: Matrix of predictors. Each row represents an observation, and each column represents a predictor. It can be a mix of continuous, discrete, and binary predictors.

y: Response variable. Can be continuous and normally distributed or binary responses.

models: A character vector specifying the types of models to include in the ensemble. Options include "elastic_net" and "random_forest". Default is "elastic_net".

k: Top number of predictors to select in the pre-screening step. Default is NULL.

r.bagging: Number of times model will perform sampling with replacement for the samples. Default is 50.

is.ensemble: Logical parameter indicating whether to perform ensemble learning of models passed as a parameter. Default is FALSE.
```{r simple_ensemble-example}
library(MASS)
data(Boston)
str(Boston)
X <- Boston[, 2:14]
y <- Boston[,1]
results <- simple_ensemble(X, y, c("elastic_net", "random_forest"), TRUE, k=8) #For Continuous Response variable
results$ensembled$random.forest$importanceSD

data(Pima.te) 
str(Pima.te)
X <- Pima.te[, 1:7]
y <- Pima.te[,8]
ensemble_predictions <- simple_ensemble(X, y, c("elastic_net"), 50, FALSE, k=4 ) #For Binary Response variable
ensemble_predictions$elastic_net
```
## Conclusion

The `simpleEnsembleGroup22` package provides tools for robust machine learning applications, integrating advanced regression and ensemble techniques to tackle complex predictive modeling tasks.
